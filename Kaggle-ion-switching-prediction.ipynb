{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, SGDRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, mean_absolute_error, make_scorer\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "import time\n",
    "import datetime\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_random = 42\n",
    "window_sizes = [10, 50] #rollong window size for signal values\n",
    "# LGB-2 model tuning\n",
    "lr_lgb = 0.05\n",
    "num_leaves = 200\n",
    "num_iterations = 2000\n",
    "# XGB-1 model tuning\n",
    "lr_xgb = 0.05\n",
    "max_depth_xgb = 10\n",
    "num_boost_round_xgb = 1000\n",
    "# Set weight of models\n",
    "w_lgb = 0.5\n",
    "w_xgb = 1 - w_lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce Memory Usage [optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        if col != 'time':\n",
    "            col_type = df[col].dtypes\n",
    "            if col_type in numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)  \n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if running code on kaggle import data from the below link\n",
    "#train = pd.read_csv('/kaggle/input/liverpool-ion-switching/train.csv')\n",
    "#test = pd.read_csv('/kaggle/input/liverpool-ion-switching/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create rolling mean,std,var,min,max for the training dataset signal values rolling widow size 10-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for window in window_sizes:\n",
    "    train[\"rolling_mean_\" + str(window)] = train['signal'].rolling(window=window).mean()\n",
    "    train[\"rolling_std_\" + str(window)] = train['signal'].rolling(window=window).std()\n",
    "    train[\"rolling_var_\" + str(window)] = train['signal'].rolling(window=window).var()\n",
    "    train[\"rolling_min_\" + str(window)] = train['signal'].rolling(window=window).min()\n",
    "    train[\"rolling_max_\" + str(window)] = train['signal'].rolling(window=window).max()\n",
    "    a = (train['signal'] - train['rolling_min_' + str(window)]) / (train['rolling_max_' + str(window)] - train['rolling_min_' + str(window)])\n",
    "    train[\"norm_\" + str(window)] = a * (np.floor(train['rolling_max_' + str(window)]) - np.ceil(train['rolling_min_' + str(window)]))  \n",
    "#replace inifinite number by nan and replace nan by zero\n",
    "train = train.replace([np.inf, -np.inf], np.nan)    \n",
    "train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat the above steps for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for window in window_sizes:\n",
    "    test[\"rolling_mean_\" + str(window)] = test['signal'].rolling(window=window).mean()\n",
    "    test[\"rolling_std_\" + str(window)] = test['signal'].rolling(window=window).std()\n",
    "    test[\"rolling_var_\" + str(window)] = test['signal'].rolling(window=window).var()\n",
    "    test[\"rolling_min_\" + str(window)] = test['signal'].rolling(window=window).min()\n",
    "    test[\"rolling_max_\" + str(window)] = test['signal'].rolling(window=window).max()\n",
    "    a = (test['signal'] - test['rolling_min_' + str(window)]) / (test['rolling_max_' + str(window)] - test['rolling_min_' + str(window)])\n",
    "    test[\"norm_\" + str(window)] = a * (np.floor(test['rolling_max_' + str(window)]) - np.ceil(test['rolling_min_' + str(window)]))\n",
    "\n",
    "test = test.replace([np.inf, -np.inf], np.nan)    \n",
    "test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def features(df):\n",
    "    df = df.sort_values(by=['time']).reset_index(drop=True)\n",
    "    df.index = ((df.time * 10_000) - 1).values\n",
    "    df['batch'] = df.index // 25_000\n",
    "    df['batch_index'] = df.index  - (df.batch * 25_000)\n",
    "    df['batch_slices'] = df['batch_index']  // 2500\n",
    "    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n",
    "    \n",
    "    for c in ['batch','batch_slices2']:\n",
    "        d = {}\n",
    "        d['mean'+c] = df.groupby([c])['signal'].mean()\n",
    "        d['median'+c] = df.groupby([c])['signal'].median()\n",
    "        d['max'+c] = df.groupby([c])['signal'].max()\n",
    "        d['min'+c] = df.groupby([c])['signal'].min()\n",
    "        d['std'+c] = df.groupby([c])['signal'].std()\n",
    "        d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n",
    "        d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))\n",
    "        d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))\n",
    "        d['range'+c] = d['max'+c] - d['min'+c]\n",
    "        d['maxtomin'+c] = d['max'+c] / d['min'+c]\n",
    "        d['abs_avg'+c] = (d['abs_min'+c] + d['abs_max'+c]) / 2\n",
    "        for v in d:\n",
    "            df[v] = df[c].map(d[v].to_dict())\n",
    "\n",
    "    \n",
    "    # add shifts_1\n",
    "    df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])\n",
    "    df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]\n",
    "    for i in df[df['batch_index']==0].index:\n",
    "        df['signal_shift_+1'][i] = np.nan\n",
    "    for i in df[df['batch_index']==49999].index:\n",
    "        df['signal_shift_-1'][i] = np.nan\n",
    "    \n",
    "    # add shifts_2 - my upgrade\n",
    "    df['signal_shift_+2'] = [0,] + [1,] + list(df['signal'].values[:-2])\n",
    "    df['signal_shift_-2'] = list(df['signal'].values[2:]) + [0] + [1]\n",
    "    for i in df[df['batch_index']==0].index:\n",
    "        df['signal_shift_+2'][i] = np.nan\n",
    "    for i in df[df['batch_index']==1].index:\n",
    "        df['signal_shift_+2'][i] = np.nan\n",
    "    for i in df[df['batch_index']==49999].index:\n",
    "        df['signal_shift_-2'][i] = np.nan\n",
    "    for i in df[df['batch_index']==49998].index:\n",
    "        df['signal_shift_-2'][i] = np.nan\n",
    "    \n",
    "    df = df.drop(columns=['batch', 'batch_index', 'batch_slices', 'batch_slices2'])\n",
    "\n",
    "    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels']]:\n",
    "        df[c+'_msignal'] = df[c] - df['signal']\n",
    "        \n",
    "    df = df.replace([np.inf, -np.inf], np.nan)    \n",
    "    df.fillna(0, inplace=True)\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "train = features(train)\n",
    "test = features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 815.39 Mb (73.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 324.25 Mb (73.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['open_channels']\n",
    "col = [c for c in train.columns if c not in ['time', 'open_channels', 'group', 'medianbatch', 'abs_avgbatch', 'abs_maxbatch']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/siavrez/simple-eda-model\n",
    "def MacroF1Metric(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n",
    "    score = f1_score(labels, preds, average = 'macro')\n",
    "    return ('MacroF1Metric', score, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[25]\tvalid_0's MacroF1Metric: 0.309632\n",
      "[50]\tvalid_0's MacroF1Metric: 0.798984\n",
      "[75]\tvalid_0's MacroF1Metric: 0.9287\n",
      "[100]\tvalid_0's MacroF1Metric: 0.932513\n",
      "[125]\tvalid_0's MacroF1Metric: 0.933809\n",
      "[150]\tvalid_0's MacroF1Metric: 0.934507\n",
      "[175]\tvalid_0's MacroF1Metric: 0.934942\n",
      "[200]\tvalid_0's MacroF1Metric: 0.935249\n",
      "[225]\tvalid_0's MacroF1Metric: 0.935448\n",
      "[250]\tvalid_0's MacroF1Metric: 0.935528\n",
      "[275]\tvalid_0's MacroF1Metric: 0.935589\n",
      "[300]\tvalid_0's MacroF1Metric: 0.935667\n",
      "[325]\tvalid_0's MacroF1Metric: 0.935693\n",
      "[350]\tvalid_0's MacroF1Metric: 0.935858\n",
      "[375]\tvalid_0's MacroF1Metric: 0.935856\n",
      "[400]\tvalid_0's MacroF1Metric: 0.935909\n",
      "[425]\tvalid_0's MacroF1Metric: 0.935947\n",
      "[450]\tvalid_0's MacroF1Metric: 0.935998\n",
      "[475]\tvalid_0's MacroF1Metric: 0.936028\n",
      "[500]\tvalid_0's MacroF1Metric: 0.936031\n",
      "[525]\tvalid_0's MacroF1Metric: 0.936039\n",
      "[550]\tvalid_0's MacroF1Metric: 0.936058\n",
      "[575]\tvalid_0's MacroF1Metric: 0.936131\n",
      "[600]\tvalid_0's MacroF1Metric: 0.936157\n",
      "[625]\tvalid_0's MacroF1Metric: 0.936152\n",
      "[650]\tvalid_0's MacroF1Metric: 0.936126\n",
      "[675]\tvalid_0's MacroF1Metric: 0.936168\n",
      "[700]\tvalid_0's MacroF1Metric: 0.936191\n",
      "[725]\tvalid_0's MacroF1Metric: 0.936221\n",
      "[750]\tvalid_0's MacroF1Metric: 0.936193\n",
      "[775]\tvalid_0's MacroF1Metric: 0.936263\n",
      "[800]\tvalid_0's MacroF1Metric: 0.936305\n",
      "[825]\tvalid_0's MacroF1Metric: 0.9363\n",
      "[850]\tvalid_0's MacroF1Metric: 0.936321\n",
      "[875]\tvalid_0's MacroF1Metric: 0.9363\n",
      "[900]\tvalid_0's MacroF1Metric: 0.936371\n",
      "[925]\tvalid_0's MacroF1Metric: 0.936396\n",
      "[950]\tvalid_0's MacroF1Metric: 0.936408\n",
      "[975]\tvalid_0's MacroF1Metric: 0.936421\n",
      "[1000]\tvalid_0's MacroF1Metric: 0.936436\n",
      "[1025]\tvalid_0's MacroF1Metric: 0.936498\n",
      "[1050]\tvalid_0's MacroF1Metric: 0.936524\n",
      "[1075]\tvalid_0's MacroF1Metric: 0.936539\n",
      "[1100]\tvalid_0's MacroF1Metric: 0.936536\n",
      "[1125]\tvalid_0's MacroF1Metric: 0.936579\n",
      "[1150]\tvalid_0's MacroF1Metric: 0.936563\n",
      "[1175]\tvalid_0's MacroF1Metric: 0.936607\n",
      "[1200]\tvalid_0's MacroF1Metric: 0.936546\n",
      "[1225]\tvalid_0's MacroF1Metric: 0.936534\n",
      "[1250]\tvalid_0's MacroF1Metric: 0.936531\n",
      "[1275]\tvalid_0's MacroF1Metric: 0.936534\n",
      "[1300]\tvalid_0's MacroF1Metric: 0.936536\n",
      "[1325]\tvalid_0's MacroF1Metric: 0.936529\n",
      "[1350]\tvalid_0's MacroF1Metric: 0.936504\n",
      "[1375]\tvalid_0's MacroF1Metric: 0.936496\n",
      "Early stopping, best iteration is:\n",
      "[1179]\tvalid_0's MacroF1Metric: 0.936617\n",
      "Wall time: 17min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Thanks to https://www.kaggle.com/jazivxt/physically-possible with tuning from https://www.kaggle.com/siavrez/simple-eda-model and my tuning\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train[col], y, test_size=0.3, random_state=seed_random)\n",
    "params = {'learning_rate': lr_lgb, \n",
    "          'max_depth': -1, \n",
    "          'num_leaves': num_leaves,\n",
    "          'metric': 'logloss', \n",
    "          'random_state': seed_random, \n",
    "          'n_jobs':-1, \n",
    "          'sample_fraction':0.33}\n",
    "model = lgb.train(params, lgb.Dataset(X_train, y_train), num_iterations, lgb.Dataset(X_valid, y_valid), verbose_eval=25, early_stopping_rounds=200, feval=MacroF1Metric)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_lgb_pred = model.predict(test[col], num_iteration=model.best_iteration)\n",
    "y_pred_train_lgb = model.predict(train[col], num_iteration=model.best_iteration)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('LGB score {0:.4f}'.format(np.mean(f1_score(y, np.round(np.clip(y_pred_train_lgb,0,10)).astype(int), average=\"macro\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# credit : taken from  https://www.kaggle.com/teejmahal20/3-simple-ideas-ensemble\n",
    "train_set = xgb.DMatrix(X_train, y_train)\n",
    "val_set = xgb.DMatrix(X_valid, y_valid)\n",
    "del X_train, X_valid, y_train, y_valid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:-2.22410\tval-logloss:-2.22342\n",
      "Multiple eval metrics have been passed: 'val-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-logloss hasn't improved in 200 rounds.\n",
      "[25]\ttrain-logloss:-72.72659\tval-logloss:-72.65256\n",
      "[50]\ttrain-logloss:-72.75009\tval-logloss:-72.69785\n",
      "[75]\ttrain-logloss:-72.75354\tval-logloss:-72.70222\n",
      "[100]\ttrain-logloss:-72.75466\tval-logloss:-72.70280\n",
      "[125]\ttrain-logloss:-72.75512\tval-logloss:-72.70273\n",
      "[150]\ttrain-logloss:-72.75542\tval-logloss:-72.70271\n",
      "[175]\ttrain-logloss:-72.75565\tval-logloss:-72.70265\n",
      "[200]\ttrain-logloss:-72.75579\tval-logloss:-72.70256\n",
      "[225]\ttrain-logloss:-72.75593\tval-logloss:-72.70245\n",
      "[250]\ttrain-logloss:-72.75597\tval-logloss:-72.70242\n",
      "[275]\ttrain-logloss:-72.75606\tval-logloss:-72.70235\n",
      "[300]\ttrain-logloss:-72.75609\tval-logloss:-72.70235\n",
      "Stopping. Best iteration:\n",
      "[101]\ttrain-logloss:-72.75468\tval-logloss:-72.70284\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_xgb = {'colsample_bytree': 0.375,\n",
    "              'learning_rate': lr_xgb,\n",
    "              'max_depth': max_depth_xgb, \n",
    "              'subsample': 1, \n",
    "              'objective':'reg:squarederror',\n",
    "              'eval_metric':'logloss'}\n",
    "\n",
    "modelx = xgb.train(params_xgb, train_set, num_boost_round=num_boost_round_xgb, evals=[(train_set, 'train'), (val_set, 'val')], \n",
    "                                     verbose_eval=25, early_stopping_rounds=200)\n",
    "del train_set, val_set\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_xgb_pred = modelx.predict(xgb.DMatrix(test[col]))\n",
    "y_pred_train_xgb = modelx.predict(xgb.DMatrix(train[col]))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('XGB score {0:.4f}'.format(np.mean(f1_score(y, np.round(np.clip(y_pred_train_xgb,0,10)).astype(int), average=\"macro\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del y_pred_train_lgb, y_pred_train_xgb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = w_lgb*y_lgb_pred + w_xgb*y_xgb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del y_lgb_pred, y_xgb_pred\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_proc(pred):\n",
    "    pred = np.round(np.clip(pred, 0, 10))\n",
    "    return pred.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction processing for the main solution\n",
    "y_preds = pred_proc(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['open_channels'] = y_preds\n",
    "test[['time','open_channels']].to_csv('submission.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "\n",
    "* [Physically Possible](https://www.kaggle.com/jazivxt/physically-possible)\n",
    "* [Simple EDA-Model](https://www.kaggle.com/siavrez/simple-eda-model)\n",
    "* [MM 2020 NCAAM: LGB, XGB, LogReg - Tuning&Merging](https://www.kaggle.com/vbmokin/mm-2020-ncaam-lgb-xgb-logreg-tuning-merging)\n",
    "* [Merging FE & Prediction - xgb, lgb, logr, linr](https://www.kaggle.com/vbmokin/merging-fe-prediction-xgb-lgb-logr-linr)\n",
    "* [BOD prediction in river - 15 regression models](https://www.kaggle.com/vbmokin/bod-prediction-in-river-15-regression-models)\n",
    "* [Automatic selection from 20 classifier models](https://www.kaggle.com/vbmokin/automatic-selection-from-20-classifier-models)\n",
    "* [3 Simple Ideas [Ensemble]](https://www.kaggle.com/teejmahal20/3-simple-ideas-ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
